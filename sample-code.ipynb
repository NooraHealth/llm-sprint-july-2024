{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40d5e54-4741-4459-a605-c4eb69a548ac",
   "metadata": {},
   "source": [
    "# Sample code for generating dummy data for monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f203a-a109-482b-a5f6-5e08ccabeb1c",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1cfb38f-a050-4670-be45-7f4bfe448600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=992969) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.llm-venv/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: ragas in ./.llm-venv/lib/python3.12/site-packages (0.1.10)\n",
      "Requirement already satisfied: numpy in ./.llm-venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in ./.llm-venv/lib/python3.12/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.llm-venv/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.llm-venv/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.llm-venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.llm-venv/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.llm-venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.llm-venv/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in ./.llm-venv/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.llm-venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in ./.llm-venv/lib/python3.12/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in ./.llm-venv/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.llm-venv/lib/python3.12/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in ./.llm-venv/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.llm-venv/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tiktoken in ./.llm-venv/lib/python3.12/site-packages (from ragas) (0.7.0)\n",
      "Requirement already satisfied: langchain in ./.llm-venv/lib/python3.12/site-packages (from ragas) (0.2.8)\n",
      "Requirement already satisfied: langchain-core in ./.llm-venv/lib/python3.12/site-packages (from ragas) (0.2.19)\n",
      "Requirement already satisfied: langchain-community in ./.llm-venv/lib/python3.12/site-packages (from ragas) (0.2.7)\n",
      "Requirement already satisfied: langchain-openai in ./.llm-venv/lib/python3.12/site-packages (from ragas) (0.1.16)\n",
      "Requirement already satisfied: openai>1 in ./.llm-venv/lib/python3.12/site-packages (from ragas) (1.35.13)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in ./.llm-venv/lib/python3.12/site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in ./.llm-venv/lib/python3.12/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in ./.llm-venv/lib/python3.12/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.llm-venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.llm-venv/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.llm-venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.llm-venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.llm-venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.llm-venv/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.llm-venv/lib/python3.12/site-packages (from openai>1->ragas) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.llm-venv/lib/python3.12/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.llm-venv/lib/python3.12/site-packages (from openai>1->ragas) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.llm-venv/lib/python3.12/site-packages (from openai>1->ragas) (2.8.2)\n",
      "Requirement already satisfied: sniffio in ./.llm-venv/lib/python3.12/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.llm-venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.llm-venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.llm-venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.llm-venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.llm-venv/lib/python3.12/site-packages (from langchain->ragas) (2.0.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.llm-venv/lib/python3.12/site-packages (from langchain->ragas) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.llm-venv/lib/python3.12/site-packages (from langchain->ragas) (0.1.86)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.llm-venv/lib/python3.12/site-packages (from langchain->ragas) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.llm-venv/lib/python3.12/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.llm-venv/lib/python3.12/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.llm-venv/lib/python3.12/site-packages (from tiktoken->ragas) (2024.5.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.llm-venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.llm-venv/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.llm-venv/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.llm-venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.llm-venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.llm-venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.llm-venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.llm-venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.llm-venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.llm-venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.llm-venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.llm-venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.llm-venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.llm-venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136f766-78f5-42cd-b901-48e752e2ad0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install langfuse pandas weaviate-client jupyter openai ragas  # or\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6f517-7b1e-47b5-9d81-d30e0083021a",
   "metadata": {},
   "source": [
    "## Setup env vars\n",
    "\n",
    "1. Create a weaviate cluster at https://console.weaviate.cloud/create-cluster \\\n",
    "   [**Note**: ingested data in free account expires after 14 days]\n",
    "2. *Wait* until the cluster is created\n",
    "3. Click on the ![Weaviate code button](weaviate-connect.svg) button to show code to connect to the cluster\n",
    "4. Add the REST endpoint into `WCS_URL` environment variable \\\n",
    "   `export WCS_URL=\"https://url.here\"`\n",
    "5. Also set the `WCS_API_KEY` environment variable to the admin API key. Click on the **API keys** button to reveal it. \\\n",
    "   `export WCS_API_KEY=\"api-key-here\"`\n",
    "6. Create new OpenAI API key at https://platform.openai.com/api-keys \\\n",
    "   `export OPENAI_API_KEY=\"key-here\"`\n",
    "7. Create new Langfuse API keys at https://cloud.langfuse.com/ (First create a project) \\\n",
    "   `export LANGFUSE_SECRET_KEY=\"secret-key\"` \\\n",
    "   `export LANGFUSE_PUBLIC_KEY=\"public-key\"` \\\n",
    "   `export LANGFUSE_HOST=\"https://cloud.langfuse.com\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a4e60-b7b1-4f50-a7b4-f2c0b4d2e759",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e8b275-4399-4569-819f-82f1ec3dd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin packages\n",
    "import os\n",
    "import json\n",
    "from textwrap import dedent\n",
    "\n",
    "# 3rd party packages\n",
    "import weaviate\n",
    "import pandas as pd\n",
    "from langfuse.openai import OpenAI\n",
    "from langfuse import Langfuse\n",
    "import langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab14f5-0758-4ddb-959d-3807752cf96f",
   "metadata": {},
   "source": [
    "## Connect to weaviate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629b22dc-a6ad-4bfd-9e7e-52d8d02a69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these environment variables\n",
    "URL = os.getenv(\"WCS_URL\")\n",
    "APIKEY = os.getenv(\"WCS_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Connect to a WCS instance\n",
    "wclient = weaviate.connect_to_wcs(\n",
    "    cluster_url=URL,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(APIKEY),\n",
    "    headers = {\n",
    "        \"X-OpenAI-Api-Key\": OPENAI_API_KEY,\n",
    "    }\n",
    ")\n",
    "lf_client = Langfuse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6006e0-8a35-41d7-9e33-409b188cc214",
   "metadata": {},
   "source": [
    "## Terms\n",
    "1. **Collection** \\\n",
    "   A collection is like a table\n",
    "2. **Field** in a collection \\\n",
    "   A field is like a column of a table\n",
    "3. **Vector** \\\n",
    "   A vector is an ordered array of numbers that represents a point in a multi-dimensional space. [Ref: ChatGPT]\n",
    "4. **Embedding** \\\n",
    "   An embedding is the mapping of an object to a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f1a0b-08f2-4df0-b430-e62e0633d40e",
   "metadata": {},
   "source": [
    "## Generating vector embeddings\n",
    "\n",
    "The following things will happen when you run the code given below:\n",
    "1. Read the `rag.csv` and do a basic cleanup\n",
    "2. Create the collection with the specified schema and OpenAI's `text-embedding-3-large` embedding model. Click [here](https://platform.openai.com/docs/guides/embeddings) for more info on other OpenAI's models.\n",
    "3. Import the records from the CSV to the newly created collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5504300f-2719-479a-a816-773806fae0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_collection(collection_name: str, *, recreate: bool) -> weaviate.collections.collection.Collection:\n",
    "    if recreate:\n",
    "        wclient.collections.delete(collection_name)\n",
    "\n",
    "    try:\n",
    "        wclient.collections.list_all()[collection_name]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        return wclient.collections.get(collection_name)\n",
    "\n",
    "    df = pd \\\n",
    "        .read_csv('rag.csv', dtype=str) \\\n",
    "        .dropna(ignore_index=True) \\\n",
    "        .drop_duplicates(subset=[\"question\"], ignore_index=True) \\\n",
    "        .map(str.strip)\n",
    "    \n",
    "    collection = wclient.collections.create(\n",
    "        name=collection_name,\n",
    "        vectorizer_config=weaviate.classes.config.Configure.Vectorizer.text2vec_openai(model=\"text-embedding-3-large\"),\n",
    "        properties=[\n",
    "            # Define db schema here\n",
    "            weaviate.classes.config.Property(name=\"question\", data_type=weaviate.classes.config.DataType.TEXT),\n",
    "            weaviate.classes.config.Property(name=\"answer\", data_type=weaviate.classes.config.DataType.TEXT),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Import the records from the CSV to our weaviate collection\n",
    "    with collection.batch.rate_limit(requests_per_minute=100) as batch:\n",
    "        for _, row in df.iterrows():\n",
    "            batch.add_object(properties=row.to_dict())\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b85f6f-493f-44f7-ac25-5b3bc677431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = get_or_create_collection(\"Medqna\", recreate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3302c-ae8c-42ae-bbbf-2d6131c5ebe5",
   "metadata": {},
   "source": [
    "## Get top 3 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24ef6c7-9a00-4a44-a175-985b8ba8c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What food to avoid during pregnancy ?.\"\n",
    "results = collection \\\n",
    "    .query \\\n",
    "    .near_text([message], limit=3) \\\n",
    "    .objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2842a5d-b341-4a05-b0bc-51227bfca03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_new = (x.properties for x in results)\n",
    "context = \"\\n\".join(\n",
    "    f'Question: \"{x['question']}\" , Answer: \"\"\"{x['answer']}\"\"\"\\n\\n'\n",
    "    for x in results_new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23cb345-41dd-4c2e-b157-a5fa5778109b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m lf_client\u001b[38;5;241m.\u001b[39mget_prompt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedical-doctor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcompile(context\u001b[38;5;241m=\u001b[39m\u001b[43mcontext\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = lf_client.get_prompt(\"medical-doctor\").compile(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53cb0c6-7fd0-47aa-ad94-6bba91e0d59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's great that you're seeking information on how to maintain a healthy diet during pregnancy. Here's what you need to know about foods to avoid during this crucial time:\n",
      "\n",
      "- Do not consume soft cheese, unpasteurized milk products, uncooked meat, fish, or eggs during pregnancy. These can contain harmful germs.\n",
      "- Always wash fruits and vegetables before eating them. Cut fruits just before you plan to eat them.\n",
      "- Avoid eating green papaya.\n",
      "- Be cautious with fish that may contain high levels of mercury, as it can harm your baby. Limit the consumption of large fish known for high mercury content.\n",
      "- Do not eat stale food, especially if it's more than a day old. If you have leftovers, store them in the fridge and reheat them thoroughly before eating.\n",
      "- Avoid drugs, narcotics, and limit your intake of tea and coffee to keep your caffeine level low.\n",
      "\n",
      "It's important to prioritize your health and the health of your baby during pregnancy. Always consult with a healthcare professional for personalized advice and recommendations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
     ]
    }
   ],
   "source": [
    "openai_client = OpenAI()\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    # model=\"gpt-4-turbo-preview\",\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\"  , \"content\": message},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a1e1b7-7315-44cd-bb11-a510f1168885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from ragas.metrics.critique import harmfulness\n",
    " \n",
    "# metrics you chose\n",
    "metrics = [faithfulness, answer_relevancy, harmfulness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af985e86-5b2b-4f63-a325-a80025924170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    " \n",
    " \n",
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985655fa-f020-4d34-b9f8-e6fab59f3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    " \n",
    "# wrappers\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    " \n",
    "llm = ChatOpenAI()\n",
    "emb = OpenAIEmbeddings()\n",
    " \n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=LangchainLLMWrapper(llm),\n",
    "    embedding=LangchainEmbeddingsWrapper(emb),\n",
    ")\n",
    "async def score_with_ragas(query, chunks, answer):\n",
    "    scores = {}\n",
    "    for m in metrics:\n",
    "        print(f\"calculating {m.name}\")\n",
    "        scores[m.name] = await m.ascore(\n",
    "            row={\"question\": query, \"contexts\": chunks, \"answer\": answer}\n",
    "        )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1d6ec8-7c59-4355-aad0-48c2a2b97a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = next(pd \\\n",
    "        .read_csv('rag.csv', dtype=str) \\\n",
    "        .dropna(ignore_index=True) \\\n",
    "        .drop_duplicates(subset=[\"question\"], ignore_index=True) \\\n",
    "        .map(str.strip)\n",
    "           .iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "012829bb-2ce6-493f-98d2-c5644410f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating faithfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating answer_relevancy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating harmfulness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/compro/tmp/llm-sprint/.llm-venv/lib/python3.12/site-packages/langfuse/task_manager.py:315: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  event[\"timestamp\"] = datetime.utcnow().replace(tzinfo=timezone.utc)\n"
     ]
    }
   ],
   "source": [
    "# start a new trace when you get a question\n",
    "question = row['question']\n",
    "trace = lf_client.trace(name = \"rag\")\n",
    " \n",
    "# retrieve the relevant chunks\n",
    "# chunks = get_similar_chunks(question)\n",
    "contexts = row['answer']\n",
    "# pass it as span\n",
    "trace.span(\n",
    "    name = \"retrieval\", input={'question': question}, output={'contexts': contexts}\n",
    ")\n",
    " \n",
    "# use llm to generate a answer with the chunks\n",
    "# answer = get_response_from_llm(question, chunks)\n",
    "answer = row['answer']\n",
    "trace.span(\n",
    "    name = \"generation\", input={'question': question, 'contexts': contexts}, output={'answer': answer}\n",
    ")\n",
    " \n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = await score_with_ragas(question, contexts, answer)\n",
    "\n",
    "# send the scores\n",
    "for m in metrics:\n",
    "    trace.score(name=m.name, value=ragas_scores[m.name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
